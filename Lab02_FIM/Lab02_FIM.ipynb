{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "DSYvP0n8RjYH"
   },
   "source": [
    "# Lab02: Frequent itemset mining\n",
    "\n",
    "- Student ID: 21424075  \n",
    "- Student name: Trần Đình Huy\n",
    "\n",
    "**How to do your homework**\n",
    "\n",
    "\n",
    "You will work directly on this notebook; the word `TODO` indicate the parts you need to do.\n",
    "\n",
    "You can discuss ideas with classmates as well as finding information from the internet, book, etc...; but *this homework must be your*.\n",
    "\n",
    "**How to submit your homework**\n",
    "\n",
    "Before submitting, rerun the notebook (`Kernel` ->` Restart & Run All`).\n",
    "\n",
    "Then create a folder named `ID` (for example, if your ID is 1234567, then name the folder `1234567`) Copy file notebook to this folder, compress and submit it on moodle.\n",
    "\n",
    "**Contents:**\n",
    "\n",
    "- Frequent itemset mining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXZ5gCVaRjYa"
   },
   "source": [
    "# 1. Preliminaries\n",
    "## This is how it all started ...\n",
    "- Rakesh Agrawal, Tomasz Imielinski, Arun N. Swami: Mining Association Rules between Sets of Items in Large Databases. SIGMOD Conference 1993: 207-216\n",
    "- Rakesh Agrawal, Ramakrishnan Srikant: Fast Algorithms for Mining Association Rules in Large Databases. VLDB 1994: 487-499\n",
    "\n",
    "**These two papers are credited with the birth of Data Mining**\n",
    "## Frequent itemset mining (FIM)\n",
    "\n",
    "Find combinations of items (itemsets) that occur frequently.\n",
    "## Applications\n",
    "- Items = products, transactions = sets of products someone bought in one trip to the store.\n",
    "$\\Rightarrow$ items people frequently buy together.\n",
    "    + Example: if people usually buy bread and coffee together, we run a sale of bread to attract people attention and raise price of coffee.\n",
    "- Items = webpages, transactions = words. Unusual words appearing together in a large number of documents, e.g., “Brad” and “Angelina,” may indicate an interesting relationship.\n",
    "- Transactions = Sentences, Items = Documents containing those sentences. Items that appear together too often could represent plagiarism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8vAJ8A2RjYi"
   },
   "source": [
    "## Transactional Database\n",
    "A transactional database $D$ consists of $N$ transactions: $D=\\left\\{T_1,T_2,...,T_N\\right\\}$. A transaction $T_n \\in D (1 \\le n \\le N)$ contains one or more items and that $I= \\left\\{ i_1,i_2,…,i_M \\right\\}$ is the set of distinct items in $D$, $T_n \\subset I$. Commonly, a transactional database is represented by a flat file instead of a database system: items are non-negative integers, each row represents a transaction, items in a transaction separated by space.\n",
    "\n",
    "Example: \n",
    "\n",
    "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 \n",
    "\n",
    "30 31 32 \n",
    "\n",
    "33 34 35 \n",
    "\n",
    "36 37 38 39 40 41 42 43 44 45 46 \n",
    "\n",
    "38 39 47 48 \n",
    "\n",
    "38 39 48 49 50 51 52 53 54 55 56 57 58 \n",
    "\n",
    "32 41 59 60 61 62 \n",
    "\n",
    "3 39 48 \n",
    "\n",
    "63 64 65 66 67 68 \n",
    "\n",
    "\n",
    "\n",
    "# Definition\n",
    "\n",
    "- Itemset: A collection of one or more items.\n",
    "    + Example: {1 4 5}\n",
    "- **k-itemset**: An itemset that contains k items.\n",
    "- Support: Frequency of occurrence of an itemset.\n",
    "    + Example: From the example above, item 3 appear in 2 transactions so its support is 2.\n",
    "- Frequent itemset: An itemset whose support is greater than or equal to a `minsup` threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdykKxr6RjY-"
   },
   "source": [
    "# The Apriori Principle\n",
    "- If an itemset is frequent, then all of its subsets must also be frequent.\n",
    "- If an itemset is not frequent, then all of its supersets cannot be frequent.\n",
    "- The support of an itemset never exceeds the support of its subsets.\n",
    "$$ \\forall{X,Y}: (X \\subseteq Y) \\Rightarrow s(X)\\ge s(Y)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvfMR7-CRjZB"
   },
   "source": [
    "# 2. Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9gZh4DORjZD"
   },
   "source": [
    "## The Apriori algorithm\n",
    "Suppose:\n",
    "\n",
    "$C_k$ candidate itemsets of size k.\n",
    "\n",
    "$L_k$ frequent itemsets of size k.\n",
    "\n",
    "The level-wise approach of Apriori algorithm can be descibed as follow:\n",
    "1. k=1, $C_k$ = all items.\n",
    "2. While $C_k$ not empty:\n",
    "    3. Scan the database to find which itemsets in $C_k$ are frequent and put them into $L_k$.\n",
    "    4. Use $L_k$ to generate a collection of candidate itemsets $C_{k+1}$ of size k+1.\n",
    "    5. k=k+1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qF9xHOBLRjZJ"
   },
   "source": [
    "### Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "7F0lUOSuRjZN"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OogwdcLRjZf"
   },
   "source": [
    "### Read data\n",
    "First we have to read data from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "U2bsGrTERjZg"
   },
   "outputs": [],
   "source": [
    "\n",
    "def readData(path):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------\n",
    "        path: path of database D.\n",
    "         \n",
    "    --------------------------\n",
    "    Returns\n",
    "        data: a dictionary for representing database D\n",
    "                 - keys: transaction tids\n",
    "                 - values: itemsets.\n",
    "        s: support of distict items in D.\n",
    "    \"\"\"\n",
    "    data={}\n",
    "    s=defaultdict(lambda: 0) # Initialize a dictionary for storing support of items in I.  \n",
    "    with open(path,'rt') as f:       \n",
    "        tid=1;\n",
    "        for line in f:           \n",
    "            itemset=set(map(int,line.split())) # a python set is a native way for storing an itemset.\n",
    "            for item in itemset:  \n",
    "                s[item]+=1     #Why don't we compute support of items while reading data?\n",
    "            data[tid]= itemset\n",
    "            tid+=1\n",
    "   \n",
    "    return data, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSTC78WURjZu"
   },
   "source": [
    "### Tree Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGAkmuXtRjZw"
   },
   "source": [
    "**I gave you pseudo code of Apriori algorithm above but we implement Tree Projection. Tell me the differences of two algorithms.**\n",
    "\n",
    "\n",
    "**TODO:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "BVRT5BnWRjZz"
   },
   "outputs": [],
   "source": [
    "def joinset(a, b):\n",
    "    '''\n",
    "    Parameters\n",
    "    -------------------\n",
    "        2 itemsets a and b (of course they are at same branch in search space)\n",
    "\n",
    "    -------------------\n",
    "    return\n",
    "        ret: itemset generated by joining a and b\n",
    "    '''\n",
    "    # TODO (hint: this function will be called in generateSearchSpace method.):\n",
    "    combined_set = set(a).union(set(b))\n",
    "    ret = combined_set\n",
    "    return ret\n",
    "\n",
    "class TP:\n",
    "    def __init__(self, data=None, s=None, minSup=None):\n",
    "        self.data = data\n",
    "        self.s = {}\n",
    "\n",
    "        for key, support in sorted(s.items(), key=lambda item: item[1]):\n",
    "            self.s[key] = support\n",
    "        # TODO: why should we do this, answer it at the markdown below?\n",
    "    \n",
    "        self.minSup = minSup\n",
    "        self.L = {}  # Store frequent itemsets mined from database\n",
    "        self.runAlgorithm()\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\"\n",
    "        Initialize search space at first step\n",
    "        --------------------------------------\n",
    "        We represent our search space in a tree structure\n",
    "        \"\"\"\n",
    "        tree = {}\n",
    "\n",
    "        search_space = {}\n",
    "        for item, support in self.s.items():\n",
    "            search_space[item] = {}\n",
    "\n",
    "            search_space[item]['itemset'] = [item]\n",
    "            ''' \n",
    "            python set does not remain elements order\n",
    "            so we use a list to extend it easily when create new itemset \n",
    "            but why we store itemset in data by a python set???? '''\n",
    "            # TODO: study about python set and its advantages,\n",
    "            # answer at the markdown below.\n",
    " \n",
    "            search_space[item]['pruned'] = False\n",
    "            # TODO:\n",
    "            # After finish implementing the algorithm tell me why should you use this\n",
    "            # instead of delete item directly from search_space and tree.\n",
    "\n",
    "            search_space[item]['support'] = support\n",
    "            print(support)\n",
    "            tree[item] = {}\n",
    "            '''\n",
    "            Why should i store an additional tree (here it called tree)? \n",
    "            Answer: This really help in next steps.\n",
    "\n",
    "            Remember that there is always a big gap from theory to practicality\n",
    "            and implementing this algorithm in python is not as simple as you think.\n",
    "            '''\n",
    "\n",
    "        return tree, search_space\n",
    "\n",
    "    def computeItemsetSupport(self, itemset):\n",
    "\n",
    "        '''Return support of itemset'''\n",
    "        # TODO (hint: this is why i use python set in data)\n",
    "        support = 0\n",
    "        for tid, transaction in self.data.items():                       \n",
    "            if {itemset}.issubset(transaction):\n",
    "                support += 1\n",
    "\n",
    "        return support \n",
    "\n",
    "    def get_sub_tree(self, k, tree, search_space, itter_node):\n",
    "        if k == 0:\n",
    "            return search_space[itter_node]['support']\n",
    "        subtree = search_space[itter_node]\n",
    "        for node in subtree.keys():\n",
    "            k-=1\n",
    "            self.get_sub_tree(k,tree,search_space,node)\n",
    "\n",
    "\n",
    "    def prune(self, k, tree, search_space):\n",
    "\n",
    "        '''\n",
    "        In this method we will find out which itemset in current search space is frequent\n",
    "        itemset then add it to L[k]. In addition, we prune those are not frequent itemsets.\n",
    "        '''\n",
    "        if self.L.get(k) is None: self.L[k] = []\n",
    "        # TODO\n",
    "        frequent_itemsets = []\n",
    "        pruned_itemsets = []\n",
    "        for itemset in search_space.keys():\n",
    "            # print(itemset)\n",
    "            support = self.computeItemsetSupport(itemset)\n",
    "           \n",
    "            if support >= self.minSup:\n",
    "                frequent_itemsets.append(itemset)\n",
    "                # print(\"sp\" , support)\n",
    "            else:\n",
    "                pruned_itemsets.append(itemset)\n",
    "\n",
    "        self.L[k] = frequent_itemsets\n",
    "        # for itemset in pruned_itemsets:\n",
    "        #     del search_space[itemset]\n",
    "        #     del tree[itemset]\n",
    "     \n",
    "\n",
    "\n",
    "    def generateSearchSpace(self, k, tree, search_space):\n",
    "        '''\n",
    "        Generate search space for exploring k+1 itemset. (Recursive function)\n",
    "        '''\n",
    "        items = list(tree.keys())\n",
    "        ''' print search_space.keys() you will understand  \n",
    "         why we need an additional tree, '''\n",
    "        l = len(items)\n",
    "        self.prune(k, tree, search_space)     \n",
    "        if l == 0: return  # Stop condition\n",
    "        for i in range(l - 1):\n",
    "            sub_search_space = {}\n",
    "            sub_tree = {}\n",
    "            a = items[i]\n",
    "            \n",
    "            \n",
    "            if search_space[a]['pruned']: continue\n",
    "            \n",
    "            for j in range(i + 1, l):\n",
    "                b = items[j]              \n",
    "                search_space[a][b] = {}\n",
    "                tree[a][b] = {}\n",
    "                # You really need to understand what am i doing here before doing work below.\n",
    "                # (Hint: draw tree and search space to draft).\n",
    "\n",
    "                # TODO:\n",
    "                # First create newset using join set\n",
    "                newset = joinset(search_space[a]['itemset'], search_space[b]['itemset'])\n",
    "            \n",
    "                # Second add newset to search_space\n",
    "                search_space[a][b] = {}\n",
    "                search_space[a][b]['itemset'] = newset\n",
    "                search_space[a][b]['pruned'] = False\n",
    "                search_space[a][b]['support'] = 0            \n",
    "                search_space[a][b] = {}\n",
    "                sub_search_space[a] = search_space[a][b]\n",
    "                sub_tree = tree[a][b]\n",
    "            #  Generate search_space for k+1-itemset\n",
    "            \n",
    "            self.generateSearchSpace(k + 1, sub_tree, sub_search_space)\n",
    "\n",
    "    def runAlgorithm(self):\n",
    "        tree, search_space = self.initialize()  # generate search space for 1-itemset\n",
    "        self.generateSearchSpace(1, tree, search_space)\n",
    "\n",
    "    def miningResults(self):\n",
    "        return self.L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tMTpwxLRjZ-"
   },
   "source": [
    "Ok, let's test on a typical dataset `chess`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "gLygYqiYRjZ-"
   },
   "outputs": [],
   "source": [
    "data, s = readData('chess.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PnxbU77YRjaF",
    "outputId": "c3b158be-6b46-4a3c-9b71-6a92d3d31ded"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "11\n",
      "15\n",
      "26\n",
      "47\n",
      "97\n",
      "120\n",
      "136\n",
      "156\n",
      "175\n",
      "175\n",
      "183\n",
      "224\n",
      "225\n",
      "322\n",
      "336\n",
      "357\n",
      "446\n",
      "482\n",
      "565\n",
      "584\n",
      "640\n",
      "696\n",
      "789\n",
      "851\n",
      "971\n",
      "980\n",
      "991\n",
      "1000\n",
      "1067\n",
      "1170\n",
      "1189\n",
      "1212\n",
      "1216\n",
      "1221\n",
      "1379\n",
      "1474\n",
      "1527\n",
      "1669\n",
      "1722\n",
      "1817\n",
      "1975\n",
      "1980\n",
      "1984\n",
      "2007\n",
      "2026\n",
      "2129\n",
      "2196\n",
      "2205\n",
      "2216\n",
      "2225\n",
      "2345\n",
      "2407\n",
      "2500\n",
      "2526\n",
      "2556\n",
      "2612\n",
      "2631\n",
      "2714\n",
      "2839\n",
      "2860\n",
      "2874\n",
      "2971\n",
      "3013\n",
      "3021\n",
      "3021\n",
      "3040\n",
      "3060\n",
      "3076\n",
      "3099\n",
      "3149\n",
      "3170\n",
      "3181\n",
      "3185\n",
      "3195\n",
      "{1: [48, 56, 66, 34, 62, 7, 36, 60, 40, 29, 52, 58], 2: [52]}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "a=TP(data=data,s=s, minSup=3000)\n",
    "print(a.miningResults())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Mp0RFbw-RjaU"
   },
   "source": [
    "### Answer questions here:\n",
    "**Why don't we compute support of items while reading data?**\n",
    "\n",
    "Trong quá trình đọc dữ liệu, chúng ta tính toán hỗ trợ của các mục riêng biệt trong từ điển `s`. Đối với mỗi mục trong mỗi itemset, chúng ta tăng giá trị hỗ trợ của mục đó trong từ điển `s`.\n",
    "\n",
    " Mục đích của việc tính toán hỗ trợ ở đây là thu thập thông tin về tần suất xuất hiện của các mục trong cơ sở dữ liệu.\n",
    "\n",
    "**why should we do sort**\n",
    "\n",
    "Việc sắp xếp và lưu trữ từ điển `s` theo `support` giúp đảm bảo rằng các mục sẽ được xử lý theo thứ tự tăng dần của `support`. \n",
    "\n",
    "Điều này quan trọng vì trong quá trình thực thi thuật toán, việc xử lý các mục theo thứ tự `support` giúp tăng hiệu suất và tính nhất quán của thuật toán.\n",
    "\n",
    " Kết quả là các itemset thường xuyên sẽ được khai thác và lưu trữ theo thứ tự hỗ trợ tăng dần trong thuộc tính self.s.\n",
    "\n",
    "**study about python set and its advantages ?**\n",
    "\n",
    "`Python set` có một số ưu điểm:\n",
    "\n",
    "1. Cấu trúc dữ liệu set trong Python giữ được tính duy nhất (unique) của các phần tử. Điều này giúp loại bỏ các phần tử trùng lặp trong itemset, vì các phần tử trong itemset thường không có tính chất thứ tự quan trọng.\n",
    "\n",
    "2. Set trong Python cung cấp các phương thức và toán tử để thực hiện các phép toán tập hợp như giao, hợp, phần tử không thuộc, kiểm tra sự thuộc, và nhiều hơn nữa. Điều này giúp cho việc xử lý itemset dễ dàng và hiệu quả.\n",
    "\n",
    "**After finish implementing the algorithm tell me why should you use this? Instead of delete item directly from search_space and tree.**\n",
    "\n",
    "Lý do sử dụng cờ 'pruned' trong search_space thay vì xóa mục trực tiếp khỏi search_space và tree là để duy trì cấu trúc và tính toàn vẹn của không gian tìm kiếm và cây trong suốt quá trình thực thi thuật toán.\n",
    "\n",
    "1. Tính linh hoạt: Cờ `pruned` cho phép chúng ta dễ dàng hoàn tác hoặc sửa đổi quyết định cắt tỉa nếu cần thiết. Thay vì xóa các mục một cách vĩnh viễn, chúng ta có thể đánh dấu chúng là đã bị cắt tỉa và sau này xem xét lại nếu cần thiết.Tính linh hoạt này là quan trọng khi triển khai các thuật toán phức tạp, nơi các điều kiện và tiêu chí khác nhau có thể ảnh hưởng đến quá trình cắt tỉa.\n",
    "\n",
    "2. Tính nhất quán dữ liệu: Không gian tìm kiếm và cây là các thành phần quan trọng của thuật toán, được sử dụng để tạo ra và khám phá các itemset. Bằng cách sử dụng cờ 'pruned', chúng ta có thể giữ cấu trúc gốc nguyên vẹn và đảm bảo rằng các hoạt động và vòng lặp tiếp theo diễn ra đúng. Nếu chúng ta xóa mục trực tiếp, có thể dẫn đến sự không nhất quán hoặc lỗi khi duyệt hoặc truy cập không gian tìm kiếm và cây.\n",
    "\n",
    "\n",
    "**Apriori algorithm and Tree Projection, tell me the differences of two algorithms.**\n",
    "\n",
    "- Giải thuật Apriori: Giải thuật Apriori là một phương pháp lặp lại sử dụng việc tạo và loại bỏ ứng viên để tìm tập hợp phổ biến. Nó tạo ra các tập hợp ứng viên có độ dài k dựa trên tập hợp phổ biến có độ dài k-1 và loại bỏ các ứng viên không đáp ứng ngưỡng hỗ trợ tối thiểu.\n",
    "\n",
    "- Giải thuật Tree Projection: Giải thuật Tree Projection, còn được gọi là FP-growth (Frequent Pattern growth), là một phương pháp chia để trị xây dựng một cấu trúc dữ liệu gọn gọi là FP-tree. Nó xây dựng FP-tree theo cách đệ quy bằng cách chiếu và gộp các tập hợp phổ biến từ tập dữ liệu. Các tập hợp phổ biến có thể được đọc trực tiếp từ FP-tree, loại bỏ nhu cầu tạo và loại bỏ ứng viên.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NnVm8wYIRjaV"
   },
   "source": [
    "# 3. Churn analysis\n",
    "\n",
    "In this section, you will use frequent itemset mining technique to analyze `churn` dataset (for any purposes). \n",
    "\n",
    "*Remember this dataset is not represented as a transactional database, first thing that you have to do is transforming it into a flat file.  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Lab01 - Frequent itemset mining.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
